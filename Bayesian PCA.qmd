---
title: "Bayesian PCA"
format: html
editor: visual
---

## Introduction

Bayesian PCA, other wise known as or with roots from factor analysis and Probabilistic PCA, is used to model latent variable in hierachical settings. The benifit of this method is as a data reduction method allowing for probabilistic inference from summarices from PCA output, estimation of complex interactions that are limited in the MLE, approach, and does well with handling missing data.

## Method

This section will first explain the notation used for Probabilistic Principal Component Analysis (PPCA) then apply a Bayesian framework to model to determine the conditional probabilities, and lastly, using variational and MCMC methods to produce estimates for the data using Stan.

### PPCA

Probabilistic PCA is similar in nature if not identical to factor anlaysis. It extends the well known method of PCA in a probabilistic setting. This allows for one to determine what variables are more probable to contribute most to the component, which is a collection of all variables, as a measure of importance, or influence, holding on to more information that could help explain clustering, outliers instead of just using the mean of the collected variables reduced to one vector.

Let there be a design matrix $X$ that is $n$ by $p$ dimensional where $i$ indexes $n$ subjects and $c$ indexes $p$ covariates. Let $\beta$ denote a $p$ by $q$ matrix of principal components (PC) vectors $q<p$ which is a set of principal components less than $p$. In the PCA and social science literature, $q =1$ is usually used to indicate the PC that explains the most variance in the data. That is, if we ordered the $q$ PC's by variance explained,indexing them by $j$, then for $q =3$, $j=1>j=2>j=3$ and if $q = p$ then all the variance of the data would be explain, thus no data reduction. Lastly, another aspect needed to map the projected observations back to the actual observations are the scores associated with each PC that is shared among all subjects. Let $z_i$ be a $q$ dimensional vector for each subject $i$.

::: callout-note
Note again that all subjects share the same scores but all have different PC loading values. Also, scores represent the standard deviation of the respected PCA. So once again, each subject gets a value along all the PC's and they are multiplied to the standard deviation of the PC as.
:::

For each subject $i$,

$$x_i=\beta z_i+\mu_p$$

or more descriptively,

$$ \begin{bmatrix}x_1\\x_2\\.\\.\\x_n  \end{bmatrix} =\begin{bmatrix}B_{11}\\B_{21}\\.\\.\\B_{p1} \end{bmatrix}z_1+\begin{bmatrix}B_{12}\\B_{22}\\.\\.\\B_{p2} \end{bmatrix}z_2+...+\begin{bmatrix}B_{1n}\\B_{2n}\\.\\.\\B_{pn} \end{bmatrix}z_n+\begin{bmatrix}\mu_1\\\mu_2\\.\\.\\\mu_p  \end{bmatrix} $$

In other words, for However, the probabilistic component comes in when we offset the above prodect of PC's and their standard deviations by the means of the

$$x_i=\beta z_i+\mu_p+\epsilon_i, \epsilon\sim N(\bf0,\sigma^2\bf I_p) $$

or more descriptively,

$$ \begin{bmatrix}x_1\\x_2\\.\\.\\x_n  \end{bmatrix} =\begin{bmatrix}B_{11}\\B_{21}\\.\\.\\B_{p1} \end{bmatrix}z_1+\begin{bmatrix}B_{12}\\B_{22}\\.\\.\\B_{p2} \end{bmatrix}z_2+...+\begin{bmatrix}B_{1n}\\B_{2n}\\.\\.\\B_{pn} \end{bmatrix}z_n+\begin{bmatrix}\mu_1\\\mu_2\\.\\.\\\mu_p  \end{bmatrix}+\begin{bmatrix} \epsilon_1\\\epsilon_2\\.\\.\\ \epsilon_n  \end{bmatrix} $$

Going forward, I will explain the method assuming $q = 1$ and $q < p$.


In order to force an order on the score, things means that ... it's no longer really standard normal prior ... but it's an order aspect. But ordering is necesary to keep track of the z's. 

Running the stand code:

```{r}

`%>%` <- dplyr::`%>%`
a= 1 # alpha for prior on the PC's
b= 1 # beta for the prior on PC's
N <- 100 # number of subjects
p <- 5 # number of covariates
q <- 2 # number from reduced dim
alpha = rgamma(1,shape =a,rate = b) # hyper prior for PC's
B = matrix(rnorm(p*q,mean = 0, sd = alpha), ncol = q) # PC Vectors
log_z <- rnorm(q, sd = .15)#PC scores
z <- exp(log_z) %>% sort(decreasing = F)
x <- matrix(nrow = N, ncol = p)

for (i in 1:p) {
  x[,i] = sum(B[i,]*z) + rnorm(N,0,1)
  
}

stan_list <- list(
  N = N, 
  a = a, 
  b = b,
  x = x,
  p = p,
  q= q
)

mod_file <- rstan::stan_model("bayes_stan.stan")

fit <- rstan::vb(mod_file,
                   data = stan_list#,
                  # pars = c("preds","B","z")
                   
                   )
```

